# GeneAgent LLM Backend Configuration
# Copy this file to .env and uncomment/modify the settings you need

# ============================================================================
# LLM Backend Selection
# ============================================================================
# Options: "azure", "ollama", "lmstudio"
# LLM_BACKEND=azure

# ============================================================================
# Azure OpenAI Configuration (when LLM_BACKEND=azure)
# ============================================================================
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2023-05-15
# AZURE_API_KEY=your-api-key-here
# AZURE_ENGINE=gpt-4o

# ============================================================================
# Ollama Configuration (when LLM_BACKEND=ollama)
# ============================================================================
# Base URL where Ollama is running (default: http://localhost:11434/v1)
# OLLAMA_BASE_URL=http://localhost:11434/v1

# Model to use (run 'ollama list' to see available models)
# Examples: llama3.1:70b, mixtral:8x7b, mistral:7b, codellama:34b
# OLLAMA_MODEL=llama3.1:70b

# ============================================================================
# LM Studio Configuration (when LLM_BACKEND=lmstudio)
# ============================================================================
# Base URL where LM Studio server is running (default: http://localhost:1234/v1)
# LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Model identifier (usually "local-model" or whatever is loaded in LM Studio)
# LMSTUDIO_MODEL=local-model

# ============================================================================
# Usage Instructions
# ============================================================================
# 1. Copy this file: cp .env.example .env
# 2. Uncomment and set the values for your chosen backend
# 3. Run: python test_config.py to verify your configuration
# 4. Run GeneAgent: python main_cascade.py
